{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Of7056nRuOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "# !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "# !apt-get update -qq 2>&1 > /dev/null\n",
        "# !apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "# creds = GoogleCredentials.get_application_default()\n",
        "# import getpass\n",
        "# !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "# vcode = getpass.getpass()\n",
        "# !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "\n",
        "# !mkdir -p Drive\n",
        "# !google-drive-ocamlfuse Drive\n",
        "!pip install transformers\n",
        "!pip install tqdm\n",
        "!pip3 install torch===1.3.1 torchvision===0.4.2 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfuEA046DAyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import datetime\n",
        "import operator\n",
        "from tqdm import tqdm\n",
        "from transformers import *\n",
        "from google.colab import files\n",
        "from torch.utils.data import DataLoader\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7vG-kXpCzAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://gitlab.com/morrisChen/irhw5.git\n",
        "filepath = \"irhw5/\"\n",
        "\n",
        "def getTrainQueryList():\n",
        "    queryListDoc = open(filepath+\"train/query_list.txt\",'r')\n",
        "    queryList=[]\n",
        "    for query in tqdm(queryListDoc):\n",
        "        queryList.append(query.strip())\n",
        "    queryListDoc.close()\n",
        "    return queryList\n",
        "\n",
        "def getTrainQueries(queryList):\n",
        "    queries=[]\n",
        "    for query in tqdm(range(len(queryList))):\n",
        "        queryQuery=open(filepath+\"train/query/\"+queryList[query],'r')\n",
        "        queries.append(queryQuery.read())\n",
        "        queryQuery.close()\n",
        "    return queries\n",
        "\n",
        "def getTrainNeg(queryList):\n",
        "    negDoc = open(filepath+\"train/Neg.txt\",'r')\n",
        "    negs = {}\n",
        "    for query in queryList:\n",
        "        negs[query]=[]\n",
        "    for neg in tqdm(negDoc):\n",
        "        neg = neg.strip().split(' ')\n",
        "        negs[neg[0]].append(neg[1])\n",
        "    negDoc.close()\n",
        "    return negs\n",
        "\n",
        "def getTrainPos(queryList):\n",
        "    posDoc = open(filepath+\"train/Pos.txt\",'r')\n",
        "    poses = {}\n",
        "    for query in queryList:\n",
        "        poses[query]=[]\n",
        "    for pos in tqdm(posDoc):\n",
        "        pos = pos.strip().split(' ')\n",
        "        poses[pos[0]].append(pos[1])\n",
        "    posDoc.close()\n",
        "    return poses\n",
        "\n",
        "def getTestQueryList():\n",
        "    queryListDoc = open(filepath+\"test/query_list.txt\",'r')\n",
        "    queryList=[]\n",
        "    for query in tqdm(queryListDoc):\n",
        "        queryList.append(query.strip())\n",
        "    queryListDoc.close()\n",
        "    return queryList\n",
        "\n",
        "def getTestQueries(queryList):\n",
        "    queries=[]\n",
        "    for query in tqdm(range(len(queryList))):\n",
        "        queryQuery=open(filepath+\"test/query/\"+queryList[query],'r')\n",
        "        queries.append(queryQuery.read())\n",
        "        queryQuery.close()\n",
        "    return queries\n",
        "        \n",
        "def getDocList():\n",
        "    #docListDoc = open(filepath+\"fileLessThan500Char.txt\",'r')\n",
        "    docListDoc = open(filepath+\"doc/hw5.txt\",'r')\n",
        "    docList=[]\n",
        "    for doc in tqdm(docListDoc):\n",
        "        docList.append(doc.strip())\n",
        "    docListDoc.close()\n",
        "    return docList\n",
        "\n",
        "def getDocs(docList):\n",
        "    docs={}\n",
        "    for doc in tqdm(range(len(docList))):\n",
        "        docDoc=open(filepath+\"doc/\"+docList[doc],'r')\n",
        "        docs[docList[doc]]=docDoc.read()\n",
        "        docDoc.close()\n",
        "    return docs\n",
        "    \n",
        "trainQueryList = getTrainQueryList()\n",
        "trainQueries = getTrainQueries(trainQueryList)\n",
        "trainQueriesNeg = getTrainNeg(trainQueryList)\n",
        "trainQueriesPos = getTrainPos(trainQueryList)\n",
        "testQueryList = getTestQueryList()\n",
        "testQueries = getTestQueries(testQueryList)\n",
        "docList = getDocList()\n",
        "docs = getDocs(docList)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJOC6ndaCzzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "def getOneStream(text_a,text_b):\n",
        "    tokens_tensor = tokenizer.encode(text_b, add_special_tokens=False)\n",
        "    clear_output(wait=True)\n",
        "    separate = math.ceil(len(tokens_tensor)/499)\n",
        "    tokens_tensors = []\n",
        "    for i in range(separate):\n",
        "        query = torch.tensor(tokenizer.encode(text_a))\n",
        "        doc = torch.tensor(tokens_tensor[i*499:(i+1)*499])\n",
        "        sep = torch.tensor([102])\n",
        "        tokens_tensors.append(torch.cat((query, doc, sep), 0))\n",
        "        #tokens_tensors.append(torch.tensor([tokenizer.encode(text_a + \"[SEP]\" + tokenizer.decode(tokens_tensor[i*499:(i+1)*499]))]))\n",
        "    return tokens_tensors\n",
        "\n",
        "# def getOneStream(text_a,text_b):\n",
        "#     tokens_tensors = []\n",
        "#     tokens_tensor = tokenizer.encode(text_a + \"[SEP]\" + text_b)\n",
        "#     return torch.tensor([tokens_tensor])\n",
        "\n",
        "def getTrainData(qidx):\n",
        "    tokens_tensors=[]\n",
        "    label_tensors=[]\n",
        "    fileNo=[]\n",
        "    text_a = trainQueries[qidx]\n",
        "    for didx in range(len(trainQueriesNeg[trainQueryList[qidx]])):\n",
        "        text_b = docs[trainQueriesNeg[trainQueryList[qidx]][didx]]\n",
        "        label_tensor = torch.tensor([0.])\n",
        "        tokens_tensor_stream = getOneStream(text_a,text_b)\n",
        "        for tensor in range(len(tokens_tensor_stream)):\n",
        "            tokens_tensors.append(tokens_tensor_stream[tensor])\n",
        "            label_tensors.append(label_tensor)\n",
        "\n",
        "    for didx in range(len(trainQueriesPos[trainQueryList[qidx]])):\n",
        "        text_b = docs[trainQueriesPos[trainQueryList[qidx]][didx]]\n",
        "        label_tensor = torch.tensor([1.])\n",
        "        tokens_tensor_stream = getOneStream(text_a,text_b)\n",
        "        for tensor in range(len(tokens_tensor_stream)):\n",
        "            tokens_tensors.append(tokens_tensor_stream[tensor])\n",
        "            label_tensors.append(label_tensor)\n",
        "        \n",
        "    return (tokens_tensors, label_tensors)\n",
        "\n",
        "# #15940 seperate to streams, each stream has BATCH doc\n",
        "# BATCH=400\n",
        "# STREAM=math.ceil(len(docList)/BATCH)\n",
        "# def getTestData(qidx,d_batch):\n",
        "#     fileName=[]\n",
        "#     tokens_tensors=[]\n",
        "#     text_a = testQueries[qidx]\n",
        "#     startIndex = d_batch*BATCH\n",
        "#     endIndex = (d_batch+1)*BATCH if (d_batch+1)*BATCH < len(docList) else len(docList)\n",
        "#     for didx in range(startIndex,endIndex):\n",
        "#         text_b = docs[didx]\n",
        "#         tokens_tensor = getOneStream(text_a,text_b)\n",
        "#         if(len(tokens_tensor[0])<510):\n",
        "#           clear_output(wait=True)\n",
        "#           tokens_tensors.append(tokens_tensor)\n",
        "#           fileName.append(docList[didx])\n",
        "#     return (tokens_tensors,fileName,startIndex)\n",
        "\n",
        "def getTestData(qidx):\n",
        "    tokens_tensors=[]\n",
        "    fileNo=[]\n",
        "    text_a = testQueries[qidx]\n",
        "    for didx in range(len(docList)):\n",
        "        text_b = docs[docList[didx]]\n",
        "        tokens_tensor_stream = getOneStream(text_a,text_b)\n",
        "        tokens_tensors.append(tokens_tensor_stream)\n",
        "    return tokens_tensors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiXjqAseHTPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleBertIR(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleBertIR,self).__init__()\n",
        "        self.bert=BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.Out_FC=torch.nn.Linear(768,1)\n",
        "        \n",
        "    def forward(self,_input_ids):\n",
        "        outputs=self.bert(_input_ids.squeeze(1))\n",
        "        CLS_representation=outputs[0][0][0]\n",
        "        Pred_out=self.Out_FC(CLS_representation)\n",
        "        return Pred_out\n",
        "    \n",
        "\n",
        "CUDA = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model=SimpleBertIR().to(CUDA)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7lwmPL_C0cx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "MSEloss=torch.nn.MSELoss()\n",
        "\n",
        "model.train()\n",
        "#for qidx in tqdm(range(10)):\n",
        "for qidx in tqdm(range(len(trainQueries))):\n",
        "    ##with torch.no_grad():\n",
        "    tokens_tensors, label_tensors = getTrainData(qidx)\n",
        "    for didx in range(len(tokens_tensors)):\n",
        "        tokens_tensor = tokens_tensors[didx]\n",
        "        label_tensor = label_tensors[didx]\n",
        "        print(tokens_tensor)\n",
        "        model_out=model(tokens_tensor.to(CUDA))\n",
        "        output=MSEloss(model_out,label_tensor.to(CUDA))\n",
        "        optimizer.zero_grad()\n",
        "        output.backward()\n",
        "        optimizer.step()\n",
        "torch.save(model, filepath+\"model.pt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP7KxlEBfUXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelload = torch.load(filepath+\"model.pt\")\n",
        "\n",
        "queryResult={}\n",
        "result={}\n",
        "with torch.no_grad():\n",
        "    for qidx in tqdm(range(len(testQueries))):\n",
        "        tokens_tensors,fileNo = getTestData(qidx)\n",
        "        for didx in range(len(tokens_tensors)):\n",
        "            tokens_tensor_streams = tokens_tensors[didx]\n",
        "            for stream in range(len(tokens_tensor_streams)):\n",
        "                result[docList[didx]] += modelload(tokens_tensor.to(CUDA))/len(tokens_tensor_streams)\n",
        "            \n",
        "        queryResult[testQueryList[qidx]] = sorted(result.items(), key=operator.itemgetter(1),reverse=True)[:100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTvZAXZxIMLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fileName = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")+\"answer.txt\"\n",
        "file= open(filepath+fileName,\"w+\")\n",
        "file.write(\"Query,RetrievedDocuments\\n\")\n",
        "for query in queryResult:\n",
        "    file.write(\"%s,\" % query)\n",
        "    for doc in range(len(queryResult[query])):\n",
        "        file.write(\" %s\" % queryResult[query][doc][0])\n",
        "    file.write(\"\\n\")\n",
        "file.close()\n",
        "files.download(filepath+fileName)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}